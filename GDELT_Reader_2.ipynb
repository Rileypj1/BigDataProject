{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disregard This:  Read Files How To\n",
    "## With Spark, you can read files directly from GDELT S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I pulled down a single day's files into an S3 bucket\n",
    "* aws s3 cp s3://gdelt-open-data/v2/events/ s3://labadie.gdelt/ --recursive --exclude \"*\" --include \"20180814??????.export.csv\"\n",
    "* I don't think you can use wildcards with hadoop distcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then brought those into hdfs\n",
    "  * hadoop distcp s3://labadie.gdelt/ gdelt/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext  \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"BDAproject\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext # Link SparkContext to SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get GDELT Events Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request  # lib that handles URLs\n",
    "#import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "target_url=\"https://raw.githubusercontent.com/linwoodc3/gdelt2HeaderRows/master/schema_csvs/GDELT_2.0_Events_Column_Labels_Header_Row_Sep2016.csv\"\n",
    "data = urllib.request.urlopen(target_url).read().decode('utf8')\n",
    "formats = pd.read_csv(io.StringIO(data))\n",
    "formats[\"dataType\"].replace({\"INTEGER\":\"IntegerType\",\n",
    "                             \"FLOAT\":\"LongType\",\n",
    "                             \"STRING\":\"StringType\"},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType\n",
    "\n",
    "gdeltschema = []\n",
    "for row in formats.iterrows():\n",
    "    sf = StructField(row[1][\"tableId\"],\n",
    "                     StringType(),\n",
    "                     #globals()[row[1][\"dataType\"]](),\n",
    "                     True)\n",
    "    gdeltschema.append(sf)\n",
    "    \n",
    "gdeltschema=StructType(gdeltschema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[GLOBALEVENTID: string, SQLDATE: string, MonthYear: string, Year: string, FractionDate: string, Actor1Code: string, Actor1Name: string, Actor1CountryCode: string, Actor1KnownGroupCode: string, Actor1EthnicCode: string, Actor1Religion1Code: string, Actor1Religion2Code: string, Actor1Type1Code: string, Actor1Type2Code: string, Actor1Type3Code: string, Actor2Code: string, Actor2Name: string, Actor2CountryCode: string, Actor2KnownGroupCode: string, Actor2EthnicCode: string, Actor2Religion1Code: string, Actor2Religion2Code: string, Actor2Type1Code: string, Actor2Type2Code: string, Actor2Type3Code: string, IsRootEvent: string, EventCode: string, EventBaseCode: string, EventRootCode: string, QuadClass: string, GoldsteinScale: string, NumMentions: string, NumSources: string, NumArticles: string, AvgTone: string, Actor1Geo_Type: string, Actor1Geo_FullName: string, Actor1Geo_CountryCode: string, Actor1Geo_ADM1Code: string, Actor1Geo_ADM2Code: string, Actor1Geo_Lat: string, Actor1Geo_Long: string, Actor1Geo_FeatureID: string, Actor2Geo_Type: string, Actor2Geo_FullName: string, Actor2Geo_CountryCode: string, Actor2Geo_ADM1Code: string, Actor2Geo_ADM2Code: string, Actor2Geo_Lat: string, Actor2Geo_Long: string, Actor2Geo_FeatureID: string, ActionGeo_Type: string, ActionGeo_FullName: string, ActionGeo_CountryCode: string, ActionGeo_ADM1Code: string, ActionGeo_ADM2Code: string, ActionGeo_Lat: string, ActionGeo_Long: string, ActionGeo_FeatureID: string, DATEADDED: string, SOURCEURL: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"delimiter\",\"\\t\") \\\n",
    "    .schema(gdeltschema) \\\n",
    "    .load(\"s3://gdelt-open-data/v2/events/20180814*.export.csv\") # Note just one day:  20180814\n",
    "df.createOrReplaceTempView(\"gdelt_df\")\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178416"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+----+------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+----------+-------------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+-----------+---------+-------------+-------------+---------+--------------+-----------+----------+-----------+-----------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+\n",
      "|GLOBALEVENTID| SQLDATE|MonthYear|Year|FractionDate|Actor1Code|Actor1Name|Actor1CountryCode|Actor1KnownGroupCode|Actor1EthnicCode|Actor1Religion1Code|Actor1Religion2Code|Actor1Type1Code|Actor1Type2Code|Actor1Type3Code|Actor2Code|   Actor2Name|Actor2CountryCode|Actor2KnownGroupCode|Actor2EthnicCode|Actor2Religion1Code|Actor2Religion2Code|Actor2Type1Code|Actor2Type2Code|Actor2Type3Code|IsRootEvent|EventCode|EventBaseCode|EventRootCode|QuadClass|GoldsteinScale|NumMentions|NumSources|NumArticles|          AvgTone|Actor1Geo_Type|  Actor1Geo_FullName|Actor1Geo_CountryCode|Actor1Geo_ADM1Code|Actor1Geo_ADM2Code|Actor1Geo_Lat|Actor1Geo_Long|Actor1Geo_FeatureID|Actor2Geo_Type|  Actor2Geo_FullName|Actor2Geo_CountryCode|Actor2Geo_ADM1Code|Actor2Geo_ADM2Code|Actor2Geo_Lat|Actor2Geo_Long|Actor2Geo_FeatureID|ActionGeo_Type|  ActionGeo_FullName|ActionGeo_CountryCode|ActionGeo_ADM1Code|ActionGeo_ADM2Code|ActionGeo_Lat|ActionGeo_Long|ActionGeo_FeatureID|     DATEADDED|           SOURCEURL|\n",
      "+-------------+--------+---------+----+------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+----------+-------------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+-----------+---------+-------------+-------------+---------+--------------+-----------+----------+-----------+-----------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+\n",
      "|    779147581|20170814|   201708|2017|   2017.6137|      null|      null|             null|                null|            null|               null|               null|           null|           null|           null|       USA|UNITED STATES|              USA|                null|            null|               null|               null|           null|           null|           null|          0|      017|          017|           01|        1|           0.0|          6|         3|          6|-2.55469415919146|             0|                null|                 null|              null|              null|         null|          null|               null|             3|White House, Dist...|                   US|              USDC|              null|      38.8951|      -77.0364|             531871|             1|      United Kingdom|                   UK|                UK|              null|           54|            -4|                 UK|20180814000000|http://www.kfvs12...|\n",
      "|    779147582|20170814|   201708|2017|   2017.6137|       AUS|QUEENSLAND|              AUS|                null|            null|               null|               null|           null|           null|           null|      null|         null|             null|                null|            null|               null|               null|           null|           null|           null|          0|      070|          070|           07|        2|           7.0|         12|         2|         12|-1.64179104477611|             4|Brisbane, Queensl...|                   AS|              AS04|            154654|        -27.5|       153.017|           -1561728|             0|                null|                 null|              null|              null|         null|          null|               null|             4|Brisbane, Queensl...|                   AS|              AS04|            154654|        -27.5|       153.017|           -1561728|20180814000000|https://www.yasst...|\n",
      "|    779147583|20170814|   201708|2017|   2017.6137|       AUS|QUEENSLAND|              AUS|                null|            null|               null|               null|           null|           null|           null|      null|         null|             null|                null|            null|               null|               null|           null|           null|           null|          0|      070|          070|           07|        2|           7.0|          4|         1|          4|-1.64179104477611|             4|Crookwell, New So...|                   AS|              AS02|            154641|     -34.4593|       149.471|           -1568133|             0|                null|                 null|              null|              null|         null|          null|               null|             4|Crookwell, New So...|                   AS|              AS02|            154641|     -34.4593|       149.471|           -1568133|20180814000000|https://www.crook...|\n",
      "|    779147584|20170814|   201708|2017|   2017.6137|       AUS|QUEENSLAND|              AUS|                null|            null|               null|               null|           null|           null|           null|      null|         null|             null|                null|            null|               null|               null|           null|           null|           null|          0|      070|          070|           07|        2|           7.0|          4|         1|          4|-1.64179104477611|             4|Yass, New South W...|                   AS|              AS02|            154643|     -34.8404|        148.91|           -1611784|             0|                null|                 null|              null|              null|         null|          null|               null|             4|Yass, New South W...|                   AS|              AS02|            154643|     -34.8404|        148.91|           -1611784|20180814000000|https://www.yasst...|\n",
      "|    779147585|20170814|   201708|2017|   2017.6137|       AUS| MELBOURNE|              AUS|                null|            null|               null|               null|           null|           null|           null|       GOV|   GOVERNMENT|             null|                null|            null|               null|               null|            GOV|           null|           null|          1|      100|          100|           10|        3|          -5.0|         10|         1|         10|-1.55038759689923|             4|Melbourne, Victor...|                   AS|              AS07|              5430|     -37.8167|       144.967|           -1586844|             4|Melbourne, Victor...|                   AS|              AS07|              5430|     -37.8167|       144.967|           -1586844|             4|Melbourne, Victor...|                   AS|              AS07|              5430|     -37.8167|       144.967|           -1586844|20180814000000|https://www.3aw.c...|\n",
      "+-------------+--------+---------+----+------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+----------+-------------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+-----------+---------+-------------+-------------+---------+--------------+-----------+----------+-----------+-----------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try some SQL and pyspark queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLOBALEVENTID| SQLDATE|MonthYear|Year|FractionDate|Actor1Code|Actor1Name|Actor1CountryCode|Actor1KnownGroupCode|\n",
    "Actor1EthnicCode|Actor1Religion1Code|Actor1Religion2Code|Actor1Type1Code|Actor1Type2Code|Actor1Type3Code|Actor2Code|\n",
    "Actor2Name|Actor2CountryCode|Actor2KnownGroupCode|Actor2EthnicCode|Actor2Religion1Code|Actor2Religion2Code|\n",
    "Actor2Type1Code|Actor2Type2Code|Actor2Type3Code|IsRootEvent|EventCode|EventBaseCode|EventRootCode|QuadClass|\n",
    "GoldsteinScale|NumMentions|NumSources|NumArticles| AvgTone|Actor1Geo_Type|  Actor1Geo_FullName|Actor1Geo_CountryCode|\n",
    "Actor1Geo_ADM1Code|Actor1Geo_ADM2Code|Actor1Geo_Lat|Actor1Geo_Long|Actor1Geo_FeatureID|Actor2Geo_Type|  \n",
    "Actor2Geo_FullName|Actor2Geo_CountryCode|Actor2Geo_ADM1Code|Actor2Geo_ADM2Code|Actor2Geo_Lat|Actor2Geo_Long|\n",
    "Actor2Geo_FeatureID|ActionGeo_Type|  ActionGeo_FullName|ActionGeo_CountryCode|ActionGeo_ADM1Code|ActionGeo_ADM2Code|\n",
    "ActionGeo_Lat|ActionGeo_Long|ActionGeo_FeatureID| DATEADDED| SOURCEURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-----------------+---------------------------+\n",
      "|min(NumMentions)|max(NumMentions)| avg(NumMentions)|count(DISTINCT NumMentions)|\n",
      "+----------------+----------------+-----------------+---------------------------+\n",
      "|               1|              99|5.064181463545871|                        108|\n",
      "+----------------+----------------+-----------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Min, Max, Avg, and distinct Count of NumMentions\n",
    "df.agg(F.min(df.NumMentions),F.max(df.NumMentions),F.avg(df.NumMentions), F.countDistinct(df.NumMentions)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+\n",
      "|MonthYear|count(DISTINCT IsRootEvent)|\n",
      "+---------+---------------------------+\n",
      "|   201808|                          2|\n",
      "|   200808|                          2|\n",
      "|   201807|                          2|\n",
      "|   201708|                          2|\n",
      "+---------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distinct Root Events by month\n",
    "df.groupBy(col(\"MonthYear\")).agg(countDistinct(col(\"IsRootEvent\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+\n",
      "|MonthYear|count(DISTINCT NumMentions)|\n",
      "+---------+---------------------------+\n",
      "|   201808|                        108|\n",
      "|   200808|                          7|\n",
      "|   201807|                         21|\n",
      "|   201708|                         28|\n",
      "+---------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distinct Mentions by month\n",
    "df.groupBy(col(\"MonthYear\")).agg(countDistinct(col(\"NumMentions\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------+\n",
      "|          Actor1Name|count(DISTINCT NumMentions)|\n",
      "+--------------------+---------------------------+\n",
      "|      LLOYDS BANKING|                          1|\n",
      "|               SAVOY|                          3|\n",
      "|                A US|                          8|\n",
      "|           TRAVELLER|                          9|\n",
      "|              SOMALI|                          9|\n",
      "|              HUNTER|                         12|\n",
      "|             ARMENIA|                         11|\n",
      "|MINIST OF GOVERNMENT|                          1|\n",
      "|               OSAGE|                          3|\n",
      "|  ANADARKO PETROLEUM|                          2|\n",
      "+--------------------+---------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distinct Mentions by Actor1Name\n",
    "df.groupBy(col(\"Actor1Name\")).agg(countDistinct(col(\"NumMentions\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          Actor1Name|count|\n",
      "+--------------------+-----+\n",
      "|              HUNTER|   61|\n",
      "|           TRAVELLER|   40|\n",
      "|              SOMALI|   37|\n",
      "|                A US|   67|\n",
      "|             ARMENIA|  120|\n",
      "|MINIST OF GOVERNMENT|    1|\n",
      "|  ANADARKO PETROLEUM|    4|\n",
      "|             GWALIOR|    1|\n",
      "|               SAVOY|    8|\n",
      "|               OSAGE|    3|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mentions by Actor1Name - slightly different from distinct count for some actors\n",
    "df.groupBy(col(\"Actor1Name\")).count().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|NumMentions|count|\n",
      "+-----------+-----+\n",
      "|          1|23902|\n",
      "|         10|26122|\n",
      "|        100|    9|\n",
      "|        102|    2|\n",
      "|        104|    3|\n",
      "|         11|  103|\n",
      "|        110|    9|\n",
      "|        115|    1|\n",
      "|        116|    1|\n",
      "|        117|    1|\n",
      "|        119|    1|\n",
      "|         12| 1272|\n",
      "|        120|   13|\n",
      "|        124|    1|\n",
      "|        125|    1|\n",
      "|        126|    1|\n",
      "|        128|    1|\n",
      "|         13|   96|\n",
      "|        130|    4|\n",
      "|        135|    1|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by and sort by number of Mentions and display count of how many times that number of mentions appears\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "df.groupBy(col(\"NumMentions\")).count().sort(\"NumMentions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|NumMentions|count|\n",
      "+-----------+-----+\n",
      "|         99|    1|\n",
      "|         98|    2|\n",
      "|         96|   11|\n",
      "|         92|    6|\n",
      "|         90|   13|\n",
      "|          9|  491|\n",
      "|         88|    1|\n",
      "|         86|    2|\n",
      "|         84|    9|\n",
      "|         83|    1|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by and sort by number of Mentions and display count of how many times that number of mentions appears\n",
    "# The orderBy descending doesn't work right - ignores 100s because they start with 1.\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "df.groupBy(col(\"NumMentions\")).count().orderBy(col(\"NumMentions\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+----+------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+----------+--------------------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+-----------+---------+-------------+-------------+---------+--------------+-----------+----------+-----------+-----------------+--------------+------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+\n",
      "|GLOBALEVENTID| SQLDATE|MonthYear|Year|FractionDate|Actor1Code|Actor1Name|Actor1CountryCode|Actor1KnownGroupCode|Actor1EthnicCode|Actor1Religion1Code|Actor1Religion2Code|Actor1Type1Code|Actor1Type2Code|Actor1Type3Code|Actor2Code|          Actor2Name|Actor2CountryCode|Actor2KnownGroupCode|Actor2EthnicCode|Actor2Religion1Code|Actor2Religion2Code|Actor2Type1Code|Actor2Type2Code|Actor2Type3Code|IsRootEvent|EventCode|EventBaseCode|EventRootCode|QuadClass|GoldsteinScale|NumMentions|NumSources|NumArticles|          AvgTone|Actor1Geo_Type|Actor1Geo_FullName|Actor1Geo_CountryCode|Actor1Geo_ADM1Code|Actor1Geo_ADM2Code|Actor1Geo_Lat|Actor1Geo_Long|Actor1Geo_FeatureID|Actor2Geo_Type|  Actor2Geo_FullName|Actor2Geo_CountryCode|Actor2Geo_ADM1Code|Actor2Geo_ADM2Code|Actor2Geo_Lat|Actor2Geo_Long|Actor2Geo_FeatureID|ActionGeo_Type|  ActionGeo_FullName|ActionGeo_CountryCode|ActionGeo_ADM1Code|ActionGeo_ADM2Code|ActionGeo_Lat|ActionGeo_Long|ActionGeo_FeatureID|     DATEADDED|           SOURCEURL|\n",
      "+-------------+--------+---------+----+------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+----------+--------------------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+-----------+---------+-------------+-------------+---------+--------------+-----------+----------+-----------+-----------------+--------------+------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+\n",
      "|    779408214|20170814|   201708|2017|   2017.6137|       AFR|    AFRICA|              AFR|                null|            null|               null|               null|           null|           null|           null|       LAB|              WORKER|             null|                null|            null|               null|               null|            LAB|           null|           null|          0|      182|          182|           18|        4|          -9.5|         10|         1|         10|-5.70522979397781|             1|             Kenya|                   KE|                KE|              null|            1|            38|                 KE|             1|               Kenya|                   KE|                KE|              null|            1|            38|                 KE|             1|               Kenya|                   KE|                KE|              null|            1|            38|                 KE|20180814234500|https://www.washi...|\n",
      "|    779408213|20170814|   201708|2017|   2017.6137|      null|      null|             null|                null|            null|               null|               null|           null|           null|           null|    GOVMIL|DEPARTMENT OF DEF...|             null|                null|            null|               null|               null|            GOV|            MIL|           null|          1|      051|          051|           05|        1|           3.4|         10|         1|         10| 1.43884892086331|             0|              null|                 null|              null|              null|         null|          null|               null|             2|North Dakota, Uni...|                   US|              USND|              null|      47.5362|       -99.793|                 ND|             2|North Dakota, Uni...|                   US|              USND|              null|      47.5362|       -99.793|                 ND|20180814234500|https://www.chron...|\n",
      "+-------------+--------+---------+----+------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+----------+--------------------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+-----------+---------+-------------+-------------+---------+--------------+-----------+----------+-----------+-----------------+--------------+------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "df.orderBy(col(\"DATEADDED\").desc()).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Spark.SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdelt_df is registered table name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   obs|\n",
      "+------+\n",
      "|178416|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count number of observations\n",
    "spark.sql(\"select count(*) as obs from gdelt_df\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Three ways to do the same thing - count number of times Actor1Name = QUEENSLAND\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col(\"Actor1Name\")=='QUEENSLAND').count()  # Use DataFrame with columns method and pyspark col function\n",
    "df.filter(df.Actor1Name==\"QUEENSLAND\").count()  # Use DataFrame columns method\n",
    "df.filter(\"Actor1Name like 'QUEENSLAND'\").count()  # Plain SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Year|num_events|\n",
      "+----+----------+\n",
      "|2008|        23|\n",
      "|2017|      1672|\n",
      "|2018|    176721|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of events by year, sorted by year\n",
    "spark.sql('SELECT Year, COUNT(GLOBALEVENTID) AS num_events FROM gdelt_df GROUP BY Year ORDER BY Year ASC').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|EventCode|num_events|\n",
      "+---------+----------+\n",
      "|      010|     14160|\n",
      "|      042|     13819|\n",
      "|      043|     12901|\n",
      "|      020|     10313|\n",
      "|      051|      9838|\n",
      "|      040|      9826|\n",
      "|      190|      7515|\n",
      "|      036|      7056|\n",
      "|      173|      6433|\n",
      "|      046|      5319|\n",
      "|      112|      3991|\n",
      "|      090|      3726|\n",
      "|      120|      3450|\n",
      "|      013|      3363|\n",
      "|      111|      3339|\n",
      "|      110|      3220|\n",
      "|      012|      3056|\n",
      "|      071|      3047|\n",
      "|      030|      2813|\n",
      "|      060|      2702|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of events by event code, sorted by number of events\n",
    "spark.sql('SELECT EventCode, COUNT(GLOBALEVENTID) AS num_events FROM gdelt_df GROUP BY EventCode ORDER BY num_events DESC').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------+----------+----------+\n",
      "|Year|EventCode|Actor1Name|Actor2Name|num_events|\n",
      "+----+---------+----------+----------+----------+\n",
      "|2018|     1246|   ARMENIA|    RUSSIA|         2|\n",
      "+----+---------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of events where actor1name and actor2name are specific values (watch case)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Year, EventCode, Actor1Name, Actor2Name, COUNT(GLOBALEVENTID) AS num_events \n",
    "    FROM gdelt_df \n",
    "    WHERE Actor1Name='ARMENIA' and Actor2Name='RUSSIA'\n",
    "    GROUP BY Year, EventCode, Actor1Name, Actor2Name\n",
    "    ORDER BY num_events DESC\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------+----------+----------+\n",
      "|Year|EventCode|Actor1Name|Actor2Name|num_events|\n",
      "+----+---------+----------+----------+----------+\n",
      "|2018|     1246|   ARMENIA|    RUSSIA|         2|\n",
      "|2018|      141|   ARMENIA|   RUSSIAN|         2|\n",
      "|2018|      043|   ARMENIA|   RUSSIAN|         1|\n",
      "+----+---------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of events where actor1name and actor2name can be fuzzy values (watch case)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Year, EventCode, Actor1Name, Actor2Name, COUNT(GLOBALEVENTID) AS num_events \n",
    "    FROM gdelt_df \n",
    "    WHERE Actor1Name='ARMENIA' and Actor2Name LIKE 'RUSSIA%'\n",
    "    GROUP BY Year, EventCode, Actor1Name, Actor2Name\n",
    "    ORDER BY num_events DESC\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Code Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### List of columns subset to return\n",
    "GLOBALEVENTID, SQLDATE, MonthYear, Year, Actor1Name, Actor2Name, IsRootEvent, EventCode, NumMentions, \n",
    "NumSources, NumArticles, AvgTone, Actor1Geo_ADM1Code, Actor2Geo_ADM1Code, ActionGeo_FullName, \n",
    "ActionGeo_CountryCode, ActionGeo_ADM1Code, DATEADDED, SOURCEURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number and avg tone of mentions from the Brexit example\n",
    "# This uses the mentions files\n",
    "\n",
    "create external table mentions (\n",
    "globaleventid INT,\n",
    "eventtimedate BIGINT,\n",
    "mentiontimedate BIGINT,\n",
    "mentiontype INT,\n",
    "mentionsourcename varchar, -- note this is a varchar although http://data.gdeltproject.org/documentation/GDELT-Event_Codebook-V2.0.pdf indicates it is an INT\n",
    "mentionidentifier varchar, -- note this is a varchar although http://data.gdeltproject.org/documentation/GDELT-Event_Codebook-V2.0.pdf indicates it is an INT\n",
    "sentenceid INT,\n",
    "actor1charoffset INT,\n",
    "actor2charoffset INT,\n",
    "actioncharoffset INT,\n",
    "inrawtext INT,\n",
    "confidence INT,\n",
    "mentiondoclen INT,\n",
    "mentiondoctone FLOAT,\n",
    "mentiondoctranslationinfo varchar,\n",
    "extras varchar\n",
    ") from s3\n",
    "target 'uri_location s3://gdelt-open-data, uri_path \"/v2/mentions/*.mentions.csv\", fmt_field_separator \"\\\\t\", ignore_invalid_records 1';\n",
    "\n",
    "create view v_mentions as select * from events;\n",
    "create view image v_mentions;\n",
    "\n",
    "\n",
    "select eventtimedate / 1000000 \"DATE\", count(*), avg(mentiondoctone) \n",
    "from v_mentions \n",
    "where mentionidentifier imatching 'brexit' \n",
    "group by 1 \n",
    "order by 1 desc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from class lab\n",
    "df1 = spark.sql(\"\"\"SELECT c1.name AS name1, c2.name AS name2, sub.charId1, sub.charId2, sub.pubCount\n",
    "FROM\n",
    "(\n",
    "  SELECT r1.charId AS charId1, r2.charId AS charId2, COUNT(r1.pubId, r2.pubId) AS pubCount\n",
    "  FROM relationships AS r1\n",
    "  CROSS JOIN relationships AS r2\n",
    "  WHERE r1.charId < r2.charId\n",
    "  AND r1.pubId=r2.pubId\n",
    "  GROUP BY r1.charId, r2.charId\n",
    ") AS sub\n",
    "INNER JOIN characters c1 ON c1.charId=sub.charId1\n",
    "INNER JOIN characters c2 ON c2.charId=sub.charId2\n",
    "ORDER BY sub.pubCount DESC\n",
    "LIMIT 10\"\"\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
